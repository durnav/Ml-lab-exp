{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcPela2L38IZm70lKGvtem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durnav/Ml-lab-exp/blob/main/HWR_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3X4-6jYbQDR"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y=load_digits().data,load_digits().target"
      ],
      "metadata": {
        "id": "vKZz_-QSbbG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRjgMX05bulb",
        "outputId": "154ffd58-173d-46e2-d63d-ecd4b012db17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
              "       ...,\n",
              "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
              "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg2n0HrPbwSD",
        "outputId": "1aea27e8-8889-4e12-f12e-794f0bdd76f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osEwDWK4bzO7",
        "outputId": "2772e88c-1018-4570-be6c-bf71c2ebff31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[0].reshape(8,8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "vIgQH4Wjb1ts",
        "outputId": "41061d27-616d-417e-9f1d-2026d2fcf63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4098df45d0>"
            ]
          },
          "metadata": {},
          "execution_count": 283
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(a):\n",
        "  e_a=np.exp(a)\n",
        "  ans=e_a/np.sum(e_a,axis=1, keepdims=True)\n",
        "  return ans"
      ],
      "metadata": {
        "id": "ak4xSDhDdC6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax([[90,10],[70,30]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl4c3FIZdWh2",
        "outputId": "a37d233c-a992-48b9-9167-8cd062c37a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e+00, 1.80485139e-35],\n",
              "       [1.00000000e+00, 4.24835426e-18]])"
            ]
          },
          "metadata": {},
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "  def __init__(self,input_size=64,layers=[500,1000],output=10):\n",
        "    np.random.seed(0)\n",
        "    model ={}\n",
        "    model['w1']=np.random.randn(input_size,layers[0])\n",
        "    model['b1']=np.zeros((1,layers[0]))\n",
        "\n",
        "    model['w2']=np.random.randn(layers[0],layers[1])\n",
        "    model['b2']=np.zeros((1,layers[1]))\n",
        "\n",
        "    model['w3']=np.random.randn(layers[1],output)\n",
        "    model['b3']=np.zeros((1,output))\n",
        "    self.model=model\n",
        "  \n",
        "  def forward(self,X):\n",
        "    z1 = np.dot(X,self.model['w1']) + self.model['b1']\n",
        "    a1=np.tanh(z1)\n",
        "\n",
        "    z2 = np.dot(a1,self.model['w2']) + self.model['b2']\n",
        "    a2=np.tanh(z2)\n",
        "\n",
        "    z3 = np.dot(a2,self.model['w3']) + self.model['b3']\n",
        "    Y_=softmax(z3)\n",
        "\n",
        "    self.activation_outputs=(a1,a2,Y_)\n",
        "    return Y_\n",
        "\n",
        "  def backward(self,X,Y,learning_rate=0.01):\n",
        "    w1,w2,w3=self.model['w1'],self.model['w2'],self.model['w3']\n",
        "    b1,b2,b3=self.model['b1'],self.model['b2'],self.model['b3']\n",
        "    m=X.shape[0]\n",
        "    a1,a2,Y_=self.activation_outputs\n",
        "\n",
        "    delta3=Y_-Y\n",
        "    dw3=np.dot(a2.T,delta3)\n",
        "    db3=np.sum(delta3, axis=0)/float(m)\n",
        "\n",
        "    delta2=(1-np.square((a2))*np.dot(delta3, w3.T))   \n",
        "    dw2=np.dot(a1.T,delta2)\n",
        "    db2=np.sum(delta2,axis=0)/float(m)\n",
        "\n",
        "    delta1=(1-np.square((a1))*np.dot(delta2,w2.T))\n",
        "    dw1=np.dot(X.T,delta1)\n",
        "    db1=np.sum(delta1,axis=0)/float(m)\n",
        "\n",
        "    self.model['w1'] -=learning_rate*dw1/m\n",
        "    self.model['b1'] -=learning_rate*db1\n",
        "\n",
        "    self.model['w2'] -=learning_rate*dw2/m\n",
        "    self.model['b2'] -=learning_rate*db2\n",
        "\n",
        "    self.model['w3'] -=learning_rate*dw3/m\n",
        "    self.model['b3'] -=learning_rate*db3\n",
        "\n",
        "  def predict(self,X):\n",
        "    Y_out=self.forward(X)\n",
        "    return np.argmax(Y_out,axis=1)\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "WKqw2B3Xdiv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(Y_oht,Y_):\n",
        "  l=-np.mean(Y_oht*np.log(Y_))\n",
        "  return l"
      ],
      "metadata": {
        "id": "hvQk6xBD5Ttd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(Y,depth):\n",
        "  m=Y.shape[0]\n",
        "  Y_oht=np.zeros((m,depth))\n",
        "  Y_oht[np.arange(m),Y]=1\n",
        "  return Y_oht"
      ],
      "metadata": {
        "id": "zxfexoSaxSvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X,Y,model,epochs,learning_rate=0.01,logs=True):\n",
        "  training_loss=[]\n",
        "  classes=10\n",
        "  Y_oht=one_hot(Y,classes)\n",
        "  for ix in range(epochs):\n",
        "    Y_=model.forward(X)\n",
        "    l=loss(Y_oht,Y_)\n",
        "    model.backward(X,Y_oht,learning_rate)\n",
        "    training_loss.append(l)\n",
        "    if(logs):\n",
        "      print(\"epochs %d Loss %.4f\" %(ix,l))\n",
        "  return training_loss"
      ],
      "metadata": {
        "id": "sjMFE0RB8gKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NN()"
      ],
      "metadata": {
        "id": "rTK7HEQE_QMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=train(X,Y,model,700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MBGqqSr_e2i",
        "outputId": "bdd1323f-e975-471c-ab77-5f5ca06d94e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 0 Loss 4.6186\n",
            "epochs 1 Loss 4.8653\n",
            "epochs 2 Loss 4.1271\n",
            "epochs 3 Loss 3.9874\n",
            "epochs 4 Loss 2.8601\n",
            "epochs 5 Loss 2.9716\n",
            "epochs 6 Loss 3.1685\n",
            "epochs 7 Loss 3.0183\n",
            "epochs 8 Loss 2.0481\n",
            "epochs 9 Loss 2.4723\n",
            "epochs 10 Loss 3.4921\n",
            "epochs 11 Loss 5.1966\n",
            "epochs 12 Loss 5.1050\n",
            "epochs 13 Loss 5.4818\n",
            "epochs 14 Loss 6.6960\n",
            "epochs 15 Loss 7.5437\n",
            "epochs 16 Loss 4.2130\n",
            "epochs 17 Loss 8.6878\n",
            "epochs 18 Loss 11.6708\n",
            "epochs 19 Loss 6.1238\n",
            "epochs 20 Loss 3.4990\n",
            "epochs 21 Loss 2.1721\n",
            "epochs 22 Loss 4.0179\n",
            "epochs 23 Loss 7.0235\n",
            "epochs 24 Loss 8.7207\n",
            "epochs 25 Loss 8.4431\n",
            "epochs 26 Loss 5.6812\n",
            "epochs 27 Loss 8.7287\n",
            "epochs 28 Loss 15.0967\n",
            "epochs 29 Loss 17.1045\n",
            "epochs 30 Loss 15.8082\n",
            "epochs 31 Loss 8.6341\n",
            "epochs 32 Loss 8.5329\n",
            "epochs 33 Loss 4.5787\n",
            "epochs 34 Loss 11.9232\n",
            "epochs 35 Loss 15.3653\n",
            "epochs 36 Loss 18.4135\n",
            "epochs 37 Loss 13.4955\n",
            "epochs 38 Loss 8.7515\n",
            "epochs 39 Loss 6.7375\n",
            "epochs 40 Loss 8.8414\n",
            "epochs 41 Loss 11.3392\n",
            "epochs 42 Loss 6.5245\n",
            "epochs 43 Loss 9.4916\n",
            "epochs 44 Loss 13.1106\n",
            "epochs 45 Loss 12.4943\n",
            "epochs 46 Loss 7.9389\n",
            "epochs 47 Loss 4.0210\n",
            "epochs 48 Loss 4.3490\n",
            "epochs 49 Loss 9.0534\n",
            "epochs 50 Loss 15.2993\n",
            "epochs 51 Loss 16.9235\n",
            "epochs 52 Loss 16.1249\n",
            "epochs 53 Loss 12.2728\n",
            "epochs 54 Loss 7.6679\n",
            "epochs 55 Loss 2.8343\n",
            "epochs 56 Loss 4.4077\n",
            "epochs 57 Loss 9.6311\n",
            "epochs 58 Loss 17.3327\n",
            "epochs 59 Loss 22.2443\n",
            "epochs 60 Loss 23.4629\n",
            "epochs 61 Loss 20.7878\n",
            "epochs 62 Loss 13.3671\n",
            "epochs 63 Loss 7.4608\n",
            "epochs 64 Loss 6.8886\n",
            "epochs 65 Loss 10.6536\n",
            "epochs 66 Loss 12.0603\n",
            "epochs 67 Loss 12.1036\n",
            "epochs 68 Loss 9.6006\n",
            "epochs 69 Loss 7.4495\n",
            "epochs 70 Loss 9.3042\n",
            "epochs 71 Loss 8.9907\n",
            "epochs 72 Loss 7.3022\n",
            "epochs 73 Loss 5.4984\n",
            "epochs 74 Loss 6.8566\n",
            "epochs 75 Loss 7.3662\n",
            "epochs 76 Loss 7.9510\n",
            "epochs 77 Loss 7.9592\n",
            "epochs 78 Loss 7.8588\n",
            "epochs 79 Loss 7.3051\n",
            "epochs 80 Loss 6.9473\n",
            "epochs 81 Loss 6.0791\n",
            "epochs 82 Loss 6.3515\n",
            "epochs 83 Loss 7.8852\n",
            "epochs 84 Loss 9.2609\n",
            "epochs 85 Loss 9.7837\n",
            "epochs 86 Loss 8.9301\n",
            "epochs 87 Loss 8.0677\n",
            "epochs 88 Loss 4.7999\n",
            "epochs 89 Loss 3.4251\n",
            "epochs 90 Loss 6.5576\n",
            "epochs 91 Loss 12.6754\n",
            "epochs 92 Loss 18.9700\n",
            "epochs 93 Loss 19.4795\n",
            "epochs 94 Loss 16.3015\n",
            "epochs 95 Loss 11.4516\n",
            "epochs 96 Loss 8.1912\n",
            "epochs 97 Loss 5.1887\n",
            "epochs 98 Loss 4.1732\n",
            "epochs 99 Loss 5.8139\n",
            "epochs 100 Loss 9.0893\n",
            "epochs 101 Loss 14.8608\n",
            "epochs 102 Loss 19.6217\n",
            "epochs 103 Loss 22.5214\n",
            "epochs 104 Loss 23.0298\n",
            "epochs 105 Loss 21.6221\n",
            "epochs 106 Loss 18.4732\n",
            "epochs 107 Loss 14.6070\n",
            "epochs 108 Loss 10.5261\n",
            "epochs 109 Loss 7.9781\n",
            "epochs 110 Loss 9.8669\n",
            "epochs 111 Loss 12.6532\n",
            "epochs 112 Loss 13.5397\n",
            "epochs 113 Loss 10.4923\n",
            "epochs 114 Loss 6.5920\n",
            "epochs 115 Loss 7.8906\n",
            "epochs 116 Loss 9.3669\n",
            "epochs 117 Loss 9.9649\n",
            "epochs 118 Loss 9.5312\n",
            "epochs 119 Loss 9.9553\n",
            "epochs 120 Loss 9.0218\n",
            "epochs 121 Loss 7.1142\n",
            "epochs 122 Loss 6.1013\n",
            "epochs 123 Loss 4.6631\n",
            "epochs 124 Loss 3.9935\n",
            "epochs 125 Loss 3.3652\n",
            "epochs 126 Loss 3.5956\n",
            "epochs 127 Loss 4.9255\n",
            "epochs 128 Loss 5.1394\n",
            "epochs 129 Loss 3.8823\n",
            "epochs 130 Loss 4.7201\n",
            "epochs 131 Loss 6.9393\n",
            "epochs 132 Loss 6.6545\n",
            "epochs 133 Loss 4.1352\n",
            "epochs 134 Loss 2.8300\n",
            "epochs 135 Loss 1.9950\n",
            "epochs 136 Loss 1.8854\n",
            "epochs 137 Loss 2.4985\n",
            "epochs 138 Loss 2.6601\n",
            "epochs 139 Loss 2.2346\n",
            "epochs 140 Loss 1.8233\n",
            "epochs 141 Loss 2.1085\n",
            "epochs 142 Loss 2.9380\n",
            "epochs 143 Loss 4.5038\n",
            "epochs 144 Loss 7.1744\n",
            "epochs 145 Loss 10.4046\n",
            "epochs 146 Loss 14.9784\n",
            "epochs 147 Loss 18.9152\n",
            "epochs 148 Loss 19.5285\n",
            "epochs 149 Loss 18.2127\n",
            "epochs 150 Loss 17.3613\n",
            "epochs 151 Loss 16.2348\n",
            "epochs 152 Loss 13.5089\n",
            "epochs 153 Loss 10.7765\n",
            "epochs 154 Loss 5.5505\n",
            "epochs 155 Loss 2.4461\n",
            "epochs 156 Loss 2.2813\n",
            "epochs 157 Loss 3.0644\n",
            "epochs 158 Loss 3.7078\n",
            "epochs 159 Loss 4.6409\n",
            "epochs 160 Loss 4.8981\n",
            "epochs 161 Loss 4.6414\n",
            "epochs 162 Loss 4.5904\n",
            "epochs 163 Loss 4.0474\n",
            "epochs 164 Loss 4.2955\n",
            "epochs 165 Loss 4.8137\n",
            "epochs 166 Loss 5.3860\n",
            "epochs 167 Loss 5.4245\n",
            "epochs 168 Loss 6.3280\n",
            "epochs 169 Loss 6.9828\n",
            "epochs 170 Loss 6.9567\n",
            "epochs 171 Loss 7.5590\n",
            "epochs 172 Loss 7.8298\n",
            "epochs 173 Loss 5.7643\n",
            "epochs 174 Loss 4.3620\n",
            "epochs 175 Loss 4.0417\n",
            "epochs 176 Loss 3.9524\n",
            "epochs 177 Loss 3.9679\n",
            "epochs 178 Loss 4.3698\n",
            "epochs 179 Loss 4.6963\n",
            "epochs 180 Loss 4.7891\n",
            "epochs 181 Loss 4.1364\n",
            "epochs 182 Loss 4.9376\n",
            "epochs 183 Loss 7.3835\n",
            "epochs 184 Loss 9.8200\n",
            "epochs 185 Loss 10.2063\n",
            "epochs 186 Loss 8.9798\n",
            "epochs 187 Loss 8.7666\n",
            "epochs 188 Loss 8.0061\n",
            "epochs 189 Loss 6.1048\n",
            "epochs 190 Loss 5.5351\n",
            "epochs 191 Loss 4.5495\n",
            "epochs 192 Loss 3.5724\n",
            "epochs 193 Loss 4.6936\n",
            "epochs 194 Loss 5.2902\n",
            "epochs 195 Loss 5.5690\n",
            "epochs 196 Loss 6.6837\n",
            "epochs 197 Loss 9.0410\n",
            "epochs 198 Loss 12.1463\n",
            "epochs 199 Loss 15.2679\n",
            "epochs 200 Loss 17.5272\n",
            "epochs 201 Loss 15.5795\n",
            "epochs 202 Loss 12.2490\n",
            "epochs 203 Loss 10.5341\n",
            "epochs 204 Loss 9.4138\n",
            "epochs 205 Loss 7.4476\n",
            "epochs 206 Loss 6.3574\n",
            "epochs 207 Loss 5.9337\n",
            "epochs 208 Loss 5.9262\n",
            "epochs 209 Loss 6.7828\n",
            "epochs 210 Loss 7.6214\n",
            "epochs 211 Loss 7.6864\n",
            "epochs 212 Loss 7.2516\n",
            "epochs 213 Loss 5.8656\n",
            "epochs 214 Loss 5.1525\n",
            "epochs 215 Loss 5.0361\n",
            "epochs 216 Loss 4.8525\n",
            "epochs 217 Loss 4.5386\n",
            "epochs 218 Loss 4.3394\n",
            "epochs 219 Loss 4.5705\n",
            "epochs 220 Loss 4.8799\n",
            "epochs 221 Loss 4.8232\n",
            "epochs 222 Loss 4.7772\n",
            "epochs 223 Loss 4.6704\n",
            "epochs 224 Loss 4.8947\n",
            "epochs 225 Loss 5.4145\n",
            "epochs 226 Loss 6.8805\n",
            "epochs 227 Loss 9.1637\n",
            "epochs 228 Loss 11.9933\n",
            "epochs 229 Loss 14.1751\n",
            "epochs 230 Loss 14.9165\n",
            "epochs 231 Loss 14.2099\n",
            "epochs 232 Loss 12.8655\n",
            "epochs 233 Loss 9.9653\n",
            "epochs 234 Loss 6.5580\n",
            "epochs 235 Loss 3.4452\n",
            "epochs 236 Loss 2.1718\n",
            "epochs 237 Loss 1.9385\n",
            "epochs 238 Loss 2.2730\n",
            "epochs 239 Loss 2.5722\n",
            "epochs 240 Loss 2.5658\n",
            "epochs 241 Loss 2.4298\n",
            "epochs 242 Loss 2.4768\n",
            "epochs 243 Loss 2.8342\n",
            "epochs 244 Loss 3.6943\n",
            "epochs 245 Loss 4.4351\n",
            "epochs 246 Loss 5.8676\n",
            "epochs 247 Loss 6.3346\n",
            "epochs 248 Loss 5.5909\n",
            "epochs 249 Loss 4.6262\n",
            "epochs 250 Loss 3.6321\n",
            "epochs 251 Loss 3.5355\n",
            "epochs 252 Loss 3.1745\n",
            "epochs 253 Loss 2.8585\n",
            "epochs 254 Loss 3.3681\n",
            "epochs 255 Loss 3.4359\n",
            "epochs 256 Loss 3.4401\n",
            "epochs 257 Loss 3.8935\n",
            "epochs 258 Loss 4.4051\n",
            "epochs 259 Loss 4.5979\n",
            "epochs 260 Loss 4.8520\n",
            "epochs 261 Loss 4.9790\n",
            "epochs 262 Loss 5.3316\n",
            "epochs 263 Loss 5.6141\n",
            "epochs 264 Loss 5.5366\n",
            "epochs 265 Loss 5.6059\n",
            "epochs 266 Loss 5.7610\n",
            "epochs 267 Loss 5.6555\n",
            "epochs 268 Loss 6.1706\n",
            "epochs 269 Loss 7.7409\n",
            "epochs 270 Loss 8.9695\n",
            "epochs 271 Loss 9.7076\n",
            "epochs 272 Loss 10.1725\n",
            "epochs 273 Loss 10.3748\n",
            "epochs 274 Loss 9.5397\n",
            "epochs 275 Loss 7.9268\n",
            "epochs 276 Loss 6.9069\n",
            "epochs 277 Loss 6.1235\n",
            "epochs 278 Loss 5.1224\n",
            "epochs 279 Loss 4.7331\n",
            "epochs 280 Loss 5.0876\n",
            "epochs 281 Loss 6.3230\n",
            "epochs 282 Loss 8.6024\n",
            "epochs 283 Loss 10.8424\n",
            "epochs 284 Loss 12.5197\n",
            "epochs 285 Loss 13.7896\n",
            "epochs 286 Loss 11.6648\n",
            "epochs 287 Loss 9.7417\n",
            "epochs 288 Loss 8.8879\n",
            "epochs 289 Loss 9.1423\n",
            "epochs 290 Loss 9.9317\n",
            "epochs 291 Loss 10.8309\n",
            "epochs 292 Loss 12.0153\n",
            "epochs 293 Loss 13.5951\n",
            "epochs 294 Loss 14.8585\n",
            "epochs 295 Loss 14.7311\n",
            "epochs 296 Loss 12.9729\n",
            "epochs 297 Loss 11.5740\n",
            "epochs 298 Loss 9.7233\n",
            "epochs 299 Loss 6.7920\n",
            "epochs 300 Loss 4.2411\n",
            "epochs 301 Loss 3.8430\n",
            "epochs 302 Loss 4.6184\n",
            "epochs 303 Loss 6.4199\n",
            "epochs 304 Loss 9.4041\n",
            "epochs 305 Loss 11.4601\n",
            "epochs 306 Loss 12.6195\n",
            "epochs 307 Loss 12.4316\n",
            "epochs 308 Loss 11.9046\n",
            "epochs 309 Loss 10.6396\n",
            "epochs 310 Loss 8.7623\n",
            "epochs 311 Loss 7.5411\n",
            "epochs 312 Loss 6.5278\n",
            "epochs 313 Loss 6.1558\n",
            "epochs 314 Loss 6.0796\n",
            "epochs 315 Loss 6.3436\n",
            "epochs 316 Loss 6.5691\n",
            "epochs 317 Loss 6.4411\n",
            "epochs 318 Loss 5.7658\n",
            "epochs 319 Loss 5.2755\n",
            "epochs 320 Loss 5.9243\n",
            "epochs 321 Loss 6.4601\n",
            "epochs 322 Loss 6.6983\n",
            "epochs 323 Loss 7.1575\n",
            "epochs 324 Loss 7.5082\n",
            "epochs 325 Loss 7.1149\n",
            "epochs 326 Loss 5.8721\n",
            "epochs 327 Loss 5.0036\n",
            "epochs 328 Loss 3.6647\n",
            "epochs 329 Loss 3.2288\n",
            "epochs 330 Loss 2.9559\n",
            "epochs 331 Loss 2.7338\n",
            "epochs 332 Loss 3.1789\n",
            "epochs 333 Loss 3.8580\n",
            "epochs 334 Loss 4.5110\n",
            "epochs 335 Loss 4.6738\n",
            "epochs 336 Loss 4.7593\n",
            "epochs 337 Loss 4.2187\n",
            "epochs 338 Loss 4.2413\n",
            "epochs 339 Loss 5.7070\n",
            "epochs 340 Loss 9.2980\n",
            "epochs 341 Loss 13.2531\n",
            "epochs 342 Loss 15.8106\n",
            "epochs 343 Loss 18.0026\n",
            "epochs 344 Loss 21.0946\n",
            "epochs 345 Loss 24.0718\n",
            "epochs 346 Loss 24.7123\n",
            "epochs 347 Loss 24.2525\n",
            "epochs 348 Loss 22.9372\n",
            "epochs 349 Loss 20.1606\n",
            "epochs 350 Loss 17.4883\n",
            "epochs 351 Loss 15.0931\n",
            "epochs 352 Loss 11.9305\n",
            "epochs 353 Loss 8.8208\n",
            "epochs 354 Loss 6.7500\n",
            "epochs 355 Loss 6.0245\n",
            "epochs 356 Loss 6.5605\n",
            "epochs 357 Loss 6.6576\n",
            "epochs 358 Loss 5.8177\n",
            "epochs 359 Loss 5.2793\n",
            "epochs 360 Loss 4.9597\n",
            "epochs 361 Loss 3.7543\n",
            "epochs 362 Loss 3.3397\n",
            "epochs 363 Loss 3.5043\n",
            "epochs 364 Loss 3.9967\n",
            "epochs 365 Loss 4.8946\n",
            "epochs 366 Loss 5.9674\n",
            "epochs 367 Loss 7.0976\n",
            "epochs 368 Loss 8.0333\n",
            "epochs 369 Loss 8.8366\n",
            "epochs 370 Loss 9.0230\n",
            "epochs 371 Loss 8.5685\n",
            "epochs 372 Loss 7.9754\n",
            "epochs 373 Loss 7.3570\n",
            "epochs 374 Loss 6.9244\n",
            "epochs 375 Loss 7.1022\n",
            "epochs 376 Loss 7.7991\n",
            "epochs 377 Loss 8.5237\n",
            "epochs 378 Loss 8.9717\n",
            "epochs 379 Loss 9.3053\n",
            "epochs 380 Loss 9.6801\n",
            "epochs 381 Loss 9.8092\n",
            "epochs 382 Loss 9.9036\n",
            "epochs 383 Loss 8.9714\n",
            "epochs 384 Loss 8.3752\n",
            "epochs 385 Loss 8.2527\n",
            "epochs 386 Loss 7.9848\n",
            "epochs 387 Loss 7.5625\n",
            "epochs 388 Loss 6.2173\n",
            "epochs 389 Loss 4.9485\n",
            "epochs 390 Loss 4.6455\n",
            "epochs 391 Loss 4.8508\n",
            "epochs 392 Loss 5.6233\n",
            "epochs 393 Loss 6.2532\n",
            "epochs 394 Loss 6.0781\n",
            "epochs 395 Loss 5.6820\n",
            "epochs 396 Loss 5.1161\n",
            "epochs 397 Loss 4.8683\n",
            "epochs 398 Loss 5.0123\n",
            "epochs 399 Loss 4.7093\n",
            "epochs 400 Loss 4.4537\n",
            "epochs 401 Loss 4.0012\n",
            "epochs 402 Loss 3.4657\n",
            "epochs 403 Loss 3.2789\n",
            "epochs 404 Loss 2.9378\n",
            "epochs 405 Loss 3.1503\n",
            "epochs 406 Loss 3.9022\n",
            "epochs 407 Loss 4.8063\n",
            "epochs 408 Loss 6.0591\n",
            "epochs 409 Loss 7.8563\n",
            "epochs 410 Loss 8.9861\n",
            "epochs 411 Loss 9.6787\n",
            "epochs 412 Loss 9.1330\n",
            "epochs 413 Loss 8.4227\n",
            "epochs 414 Loss 7.7851\n",
            "epochs 415 Loss 7.0529\n",
            "epochs 416 Loss 6.0053\n",
            "epochs 417 Loss 5.8326\n",
            "epochs 418 Loss 5.8901\n",
            "epochs 419 Loss 7.1100\n",
            "epochs 420 Loss 7.6518\n",
            "epochs 421 Loss 6.6920\n",
            "epochs 422 Loss 6.0229\n",
            "epochs 423 Loss 5.3633\n",
            "epochs 424 Loss 4.0532\n",
            "epochs 425 Loss 3.0681\n",
            "epochs 426 Loss 3.0162\n",
            "epochs 427 Loss 3.6447\n",
            "epochs 428 Loss 4.6639\n",
            "epochs 429 Loss 5.7870\n",
            "epochs 430 Loss 6.8677\n",
            "epochs 431 Loss 7.7819\n",
            "epochs 432 Loss 8.3655\n",
            "epochs 433 Loss 9.1632\n",
            "epochs 434 Loss 9.5060\n",
            "epochs 435 Loss 10.1157\n",
            "epochs 436 Loss 10.5301\n",
            "epochs 437 Loss 10.9942\n",
            "epochs 438 Loss 11.4682\n",
            "epochs 439 Loss 11.2636\n",
            "epochs 440 Loss 10.7795\n",
            "epochs 441 Loss 9.7195\n",
            "epochs 442 Loss 8.9614\n",
            "epochs 443 Loss 7.9045\n",
            "epochs 444 Loss 7.0246\n",
            "epochs 445 Loss 6.7934\n",
            "epochs 446 Loss 6.7845\n",
            "epochs 447 Loss 6.9515\n",
            "epochs 448 Loss 7.4182\n",
            "epochs 449 Loss 8.4424\n",
            "epochs 450 Loss 8.0953\n",
            "epochs 451 Loss 7.3423\n",
            "epochs 452 Loss 6.6739\n",
            "epochs 453 Loss 6.1187\n",
            "epochs 454 Loss 6.8310\n",
            "epochs 455 Loss 7.3168\n",
            "epochs 456 Loss 7.0319\n",
            "epochs 457 Loss 6.5649\n",
            "epochs 458 Loss 6.5641\n",
            "epochs 459 Loss 6.8749\n",
            "epochs 460 Loss 7.4774\n",
            "epochs 461 Loss 7.9944\n",
            "epochs 462 Loss 8.4773\n",
            "epochs 463 Loss 9.0727\n",
            "epochs 464 Loss 9.1586\n",
            "epochs 465 Loss 8.9981\n",
            "epochs 466 Loss 8.0858\n",
            "epochs 467 Loss 7.2701\n",
            "epochs 468 Loss 6.5532\n",
            "epochs 469 Loss 6.4842\n",
            "epochs 470 Loss 6.3928\n",
            "epochs 471 Loss 5.3873\n",
            "epochs 472 Loss 3.9919\n",
            "epochs 473 Loss 3.0303\n",
            "epochs 474 Loss 2.5210\n",
            "epochs 475 Loss 2.5084\n",
            "epochs 476 Loss 2.7123\n",
            "epochs 477 Loss 3.1028\n",
            "epochs 478 Loss 3.5679\n",
            "epochs 479 Loss 3.7149\n",
            "epochs 480 Loss 3.8405\n",
            "epochs 481 Loss 4.1907\n",
            "epochs 482 Loss 5.1828\n",
            "epochs 483 Loss 6.3738\n",
            "epochs 484 Loss 7.9799\n",
            "epochs 485 Loss 9.4135\n",
            "epochs 486 Loss 10.0945\n",
            "epochs 487 Loss 9.8746\n",
            "epochs 488 Loss 8.7368\n",
            "epochs 489 Loss 7.9039\n",
            "epochs 490 Loss 7.3857\n",
            "epochs 491 Loss 6.7201\n",
            "epochs 492 Loss 6.0467\n",
            "epochs 493 Loss 5.5750\n",
            "epochs 494 Loss 5.5329\n",
            "epochs 495 Loss 5.4962\n",
            "epochs 496 Loss 5.1951\n",
            "epochs 497 Loss 5.1578\n",
            "epochs 498 Loss 5.1859\n",
            "epochs 499 Loss 5.4599\n",
            "epochs 500 Loss 5.4106\n",
            "epochs 501 Loss 5.1635\n",
            "epochs 502 Loss 4.8886\n",
            "epochs 503 Loss 4.8382\n",
            "epochs 504 Loss 4.6823\n",
            "epochs 505 Loss 4.6913\n",
            "epochs 506 Loss 4.9593\n",
            "epochs 507 Loss 5.2467\n",
            "epochs 508 Loss 5.6846\n",
            "epochs 509 Loss 5.8544\n",
            "epochs 510 Loss 5.9825\n",
            "epochs 511 Loss 5.8725\n",
            "epochs 512 Loss 6.3419\n",
            "epochs 513 Loss 6.3104\n",
            "epochs 514 Loss 6.4561\n",
            "epochs 515 Loss 6.2094\n",
            "epochs 516 Loss 6.1760\n",
            "epochs 517 Loss 5.8779\n",
            "epochs 518 Loss 5.8278\n",
            "epochs 519 Loss 5.7752\n",
            "epochs 520 Loss 5.9558\n",
            "epochs 521 Loss 6.3651\n",
            "epochs 522 Loss 7.3915\n",
            "epochs 523 Loss 8.3605\n",
            "epochs 524 Loss 8.8930\n",
            "epochs 525 Loss 9.3618\n",
            "epochs 526 Loss 9.5286\n",
            "epochs 527 Loss 10.0111\n",
            "epochs 528 Loss 10.7708\n",
            "epochs 529 Loss 11.8092\n",
            "epochs 530 Loss 11.5057\n",
            "epochs 531 Loss 9.9596\n",
            "epochs 532 Loss 8.1440\n",
            "epochs 533 Loss 7.2384\n",
            "epochs 534 Loss 6.6164\n",
            "epochs 535 Loss 5.6524\n",
            "epochs 536 Loss 5.1076\n",
            "epochs 537 Loss 4.6549\n",
            "epochs 538 Loss 4.3468\n",
            "epochs 539 Loss 4.4027\n",
            "epochs 540 Loss 4.7937\n",
            "epochs 541 Loss 5.0536\n",
            "epochs 542 Loss 5.2927\n",
            "epochs 543 Loss 5.5213\n",
            "epochs 544 Loss 5.1034\n",
            "epochs 545 Loss 4.8200\n",
            "epochs 546 Loss 5.5540\n",
            "epochs 547 Loss 6.9425\n",
            "epochs 548 Loss 8.0584\n",
            "epochs 549 Loss 9.0586\n",
            "epochs 550 Loss 9.8167\n",
            "epochs 551 Loss 10.2841\n",
            "epochs 552 Loss 10.1002\n",
            "epochs 553 Loss 10.0127\n",
            "epochs 554 Loss 9.5052\n",
            "epochs 555 Loss 9.1924\n",
            "epochs 556 Loss 8.4617\n",
            "epochs 557 Loss 8.4190\n",
            "epochs 558 Loss 8.5472\n",
            "epochs 559 Loss 8.2585\n",
            "epochs 560 Loss 8.1820\n",
            "epochs 561 Loss 7.9713\n",
            "epochs 562 Loss 7.5826\n",
            "epochs 563 Loss 7.4864\n",
            "epochs 564 Loss 7.0580\n",
            "epochs 565 Loss 6.6636\n",
            "epochs 566 Loss 6.0459\n",
            "epochs 567 Loss 5.5865\n",
            "epochs 568 Loss 5.4825\n",
            "epochs 569 Loss 5.5390\n",
            "epochs 570 Loss 5.6609\n",
            "epochs 571 Loss 5.1874\n",
            "epochs 572 Loss 4.2089\n",
            "epochs 573 Loss 3.5531\n",
            "epochs 574 Loss 3.2584\n",
            "epochs 575 Loss 3.2150\n",
            "epochs 576 Loss 3.4891\n",
            "epochs 577 Loss 3.6274\n",
            "epochs 578 Loss 3.8552\n",
            "epochs 579 Loss 4.3381\n",
            "epochs 580 Loss 4.6220\n",
            "epochs 581 Loss 4.8745\n",
            "epochs 582 Loss 5.3137\n",
            "epochs 583 Loss 6.0598\n",
            "epochs 584 Loss 6.9680\n",
            "epochs 585 Loss 7.5223\n",
            "epochs 586 Loss 7.9029\n",
            "epochs 587 Loss 7.6614\n",
            "epochs 588 Loss 7.4766\n",
            "epochs 589 Loss 7.0525\n",
            "epochs 590 Loss 6.3276\n",
            "epochs 591 Loss 5.8174\n",
            "epochs 592 Loss 5.7997\n",
            "epochs 593 Loss 5.6278\n",
            "epochs 594 Loss 5.5810\n",
            "epochs 595 Loss 5.3765\n",
            "epochs 596 Loss 5.0746\n",
            "epochs 597 Loss 4.8926\n",
            "epochs 598 Loss 4.8739\n",
            "epochs 599 Loss 4.9884\n",
            "epochs 600 Loss 5.0198\n",
            "epochs 601 Loss 4.8206\n",
            "epochs 602 Loss 3.7950\n",
            "epochs 603 Loss 3.0386\n",
            "epochs 604 Loss 2.6330\n",
            "epochs 605 Loss 2.3615\n",
            "epochs 606 Loss 2.1731\n",
            "epochs 607 Loss 2.1332\n",
            "epochs 608 Loss 2.5200\n",
            "epochs 609 Loss 3.3066\n",
            "epochs 610 Loss 4.3171\n",
            "epochs 611 Loss 5.5079\n",
            "epochs 612 Loss 7.2942\n",
            "epochs 613 Loss 8.9544\n",
            "epochs 614 Loss 9.7676\n",
            "epochs 615 Loss 11.2628\n",
            "epochs 616 Loss 12.0098\n",
            "epochs 617 Loss 11.6004\n",
            "epochs 618 Loss 9.4562\n",
            "epochs 619 Loss 7.7954\n",
            "epochs 620 Loss 7.0092\n",
            "epochs 621 Loss 6.3086\n",
            "epochs 622 Loss 6.0688\n",
            "epochs 623 Loss 5.8912\n",
            "epochs 624 Loss 6.0834\n",
            "epochs 625 Loss 6.3821\n",
            "epochs 626 Loss 6.6980\n",
            "epochs 627 Loss 6.8039\n",
            "epochs 628 Loss 6.5434\n",
            "epochs 629 Loss 6.1680\n",
            "epochs 630 Loss 6.1388\n",
            "epochs 631 Loss 6.2986\n",
            "epochs 632 Loss 6.7284\n",
            "epochs 633 Loss 6.8580\n",
            "epochs 634 Loss 7.1771\n",
            "epochs 635 Loss 7.0608\n",
            "epochs 636 Loss 7.1017\n",
            "epochs 637 Loss 7.6340\n",
            "epochs 638 Loss 8.1182\n",
            "epochs 639 Loss 8.3643\n",
            "epochs 640 Loss 8.1539\n",
            "epochs 641 Loss 7.8792\n",
            "epochs 642 Loss 8.0686\n",
            "epochs 643 Loss 8.3651\n",
            "epochs 644 Loss 8.4295\n",
            "epochs 645 Loss 8.1024\n",
            "epochs 646 Loss 7.6538\n",
            "epochs 647 Loss 7.6534\n",
            "epochs 648 Loss 7.5127\n",
            "epochs 649 Loss 7.5716\n",
            "epochs 650 Loss 7.5487\n",
            "epochs 651 Loss 7.7549\n",
            "epochs 652 Loss 7.9179\n",
            "epochs 653 Loss 8.1752\n",
            "epochs 654 Loss 8.8146\n",
            "epochs 655 Loss 9.3636\n",
            "epochs 656 Loss 9.7394\n",
            "epochs 657 Loss 9.9032\n",
            "epochs 658 Loss 9.5138\n",
            "epochs 659 Loss 9.1689\n",
            "epochs 660 Loss 8.3826\n",
            "epochs 661 Loss 7.6491\n",
            "epochs 662 Loss 7.3284\n",
            "epochs 663 Loss 7.1384\n",
            "epochs 664 Loss 7.0076\n",
            "epochs 665 Loss 6.8393\n",
            "epochs 666 Loss 6.6843\n",
            "epochs 667 Loss 6.3369\n",
            "epochs 668 Loss 5.9312\n",
            "epochs 669 Loss 5.7910\n",
            "epochs 670 Loss 5.9646\n",
            "epochs 671 Loss 5.9610\n",
            "epochs 672 Loss 5.8362\n",
            "epochs 673 Loss 5.5929\n",
            "epochs 674 Loss 5.0391\n",
            "epochs 675 Loss 4.7546\n",
            "epochs 676 Loss 4.7028\n",
            "epochs 677 Loss 4.6664\n",
            "epochs 678 Loss 5.0500\n",
            "epochs 679 Loss 5.1022\n",
            "epochs 680 Loss 4.5616\n",
            "epochs 681 Loss 4.4634\n",
            "epochs 682 Loss 4.5868\n",
            "epochs 683 Loss 4.4764\n",
            "epochs 684 Loss 3.9973\n",
            "epochs 685 Loss 3.6138\n",
            "epochs 686 Loss 3.3404\n",
            "epochs 687 Loss 3.1320\n",
            "epochs 688 Loss 2.9384\n",
            "epochs 689 Loss 2.7655\n",
            "epochs 690 Loss 2.3026\n",
            "epochs 691 Loss 2.0769\n",
            "epochs 692 Loss 2.1684\n",
            "epochs 693 Loss 2.4937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred=model.predict(X)\n"
      ],
      "metadata": {
        "id": "98Ye1-HBK259"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(Ypred == Y)"
      ],
      "metadata": {
        "id": "192XTydBMMrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[20].reshape(8,8))"
      ],
      "metadata": {
        "id": "0FH8jH8NMR6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred[20]"
      ],
      "metadata": {
        "id": "2gPS8oR4MuuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y[20]"
      ],
      "metadata": {
        "id": "xZ2pncEwM1pk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}